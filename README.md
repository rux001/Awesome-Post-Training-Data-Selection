# Awesome-Post-Training-Data-Selection

A curated list of research papers on data selection methods for instruction tuning of Large Language Models.

## ðŸ“‘ Table of Contents
- [Data Quality Assessment](#data-quality-assessment)
- [Iterative Selection Methods](#iterative-selection-methods)
- [Model-Based Selection](#model-based-selection)
- [Diversity-Based Methods](#diversity-based-methods)
- [Self-Reflection and Uncertainty](#self-reflection-and-uncertainty)
- [Gradient-Based Methods](#gradient-based-methods)
- [Influence-Based Methods](#influence-based-methods)
- [External Knowledge Integration](#external-knowledge-integration)

---

## Data Quality Assessment

### Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning
**Authors:** Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning Cheng, Tianyi Zhou  
**Venue:** Annual Meeting of the Association for Computational Linguistics (ACL) 2024  
**Paper:** [Link needed]  
**Code:** [Link needed]  
**Summary:** Proposes a weak-to-strong filtering approach for efficient instruction tuning data selection.

### From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning
**Authors:** Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, Jing Xiao  
**Venue:** North American Chapter of the Association for Computational Linguistics 2023  
**Paper:** [Link needed]  
**Code:** [Link needed]  
**Summary:** Focuses on quality over quantity through self-guided data selection mechanisms.

---

## Iterative Selection Methods

### IterSelectTune: An Iterative Training Framework for Efficient Instruction-Tuning Data Selection
**Authors:** Jielin Song, Siyu Liu, Bin Zhu, Yanghui Rao  
**Venue:** arXiv 2024  
**Paper:** [Link needed]  
**Code:** [Link needed]  
**Summary:** Presents an iterative framework for progressively refining instruction tuning data selection.

### LEAD: Iterative Data Selection for Efficient LLM Instruction Tuning
**Authors:** Xiaotian Lin, Yanlin Qi, Yizhang Zhu, Themis Palpanas, Chengliang Chai, Nan Tang, Yuyu Luo  
**Venue:** arXiv 2025  
**Paper:** [Link needed]  
**Code:** [Link needed]  
**Summary:** Iterative approach for efficient data selection in LLM instruction tuning.

---

## Model-Based Selection

### Smaller Language Models are capable of selecting Instruction-Tuning Training Data for Larger Language Models
**Authors:** Dheeraj Mekala, Alex Nguyen, Jingbo Shang  
**Venue:** Annual Meeting of the Association for Computational Linguistics 2024  
**Paper:** [Link needed]  
**Code:** [Link needed]  
**Summary:** Demonstrates how smaller models can effectively select training data for larger models.

### MoDS: Model-oriented Data Selection for Instruction Tuning
**Authors:** Qianlong Du, Chengqing Zong, Jiajun Zhang  
**Venue:** arXiv 2023  
**Paper:** [Link needed]  
**Code:** [Link needed]  
**Summary:** Model-oriented approach to data selection for instruction tuning.

---

## Diversity-Based Methods

### Self-Evolved Diverse Data Sampling for Efficient Instruction Tuning
**Authors:** Shengguang Wu, Keming Lu, Benfeng Xu, Junyang Lin, Qi Su, Chang Zhou  
**Venue:** arXiv 2023  
**Paper:** [Link needed]  
**Code:** [Link needed]  
**Summary:** Self-evolutionary approach for diverse data sampling in instruction tuning.

### Data Diversity Matters for Robust Instruction Tuning
**Authors:** Alexander Bukharin, Shiyang Li, Zhengyang Wang, Jingfeng Yang, Bing Yin, Xian Li, Chao Zhang, Tuo Zhao, Haoming Jiang  
**Venue:** Conference on Empirical Methods in Natural Language Processing (EMNLP) 2024  
**Paper:** [Link needed]  
**Code:** [Link needed]  
**Summary:** Investigates the importance of data diversity for robust instruction tuning.

### Diversity Measurement and Subset Selection for Instruction Tuning Datasets
**Authors:** Peiqi Wang, Yikang Shen, Zhen Guo, Matt Stallone, Yoon Kim, Polina Golland, Rameswar Panda  
**Venue:** arXiv 2024  
**Paper:** [Link needed]  
**Code:** [Link needed]  
**Summary:** Methods for measuring diversity and selecting subsets in instruction tuning datasets.

### Improving Influence-based Instruction Tuning Data Selection for Balanced Learning of Diverse Capabilities
**Authors:** Qirun Dai, Dylan Zhang, Jiaqi W. Ma, Hao Peng  
**Venue:** arXiv 2025  
**Paper:** [Link needed]  
**Code:** [Link needed]  
**Summary:** Influence-based methods for balanced learning across diverse capabilities.

---

## Self-Reflection and Uncertainty

### SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware Self-Reflection
**Authors:** Liangxin Liu, Xuebo Liu, Derek F. Wong, Dongfang Li, Ziyi Wang, Baotian Hu, Min Zhang  
**Venue:** Conference on Neural Information Processing Systems (NeurIPS) 2024  
**Paper:** [Link needed]  
**Code:** [Link needed]  
**Summary:** Uses uncertainty-aware self-reflection for selective instruction tuning.

### Selective Reflection-Tuning: Student-Selected Data Recycling for LLM Instruction-Tuning
**Authors:** Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Jiuxiang Gu, Tianyi Zhou  
**Venue:** Annual Meeting of the Association for Computational Linguistics 2024  
**Paper:** [Link needed]  
**Code:** [Link needed]  
**Summary:** Student-selected data recycling approach for instruction tuning.

### Reflection-Tuning: Data Recycling Improves LLM Instruction-Tuning
**Authors:** Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Heng Huang, Jiuxiang Gu, Tianyi Zhou  
**Venue:** arXiv 2023  
**Paper:** [Link needed]  
**Code:** [Link needed]  
**Summary:** Data recycling through reflection for improved instruction tuning.

---

## Gradient-Based Methods

### Beyond Similarity: A Gradient-based Graph Method for Instruction Tuning Data Selection
**Authors:** Yang Zhao, Li Du, Xiao Ding, Yangou Ouyang, Hepeng Wang, Kai Xiong, Jin-Fang Gao, Zhouhao Sun, Dongliang Xu, Yang Qing, Dongcheng Li, Bing Qin, Ting Liu  
**Venue:** arXiv 2025  
**Paper:** [Link needed]  
**Code:** [Link needed]  
**Summary:** Gradient-based graph methods going beyond similarity for data selection.

---

## Influence-Based Methods

### LESS: Selecting Influential Data for Targeted Instruction Tuning
**Authors:** Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, Danqi Chen  
**Venue:** ICLR 2024 Workshop on Navigating and Addressing Data Problems for Foundation Models  
**Paper:** [Link needed]  
**Code:** [Link needed]  
**Summary:** Selecting influential data for targeted instruction tuning.

---

## External Knowledge Integration

### RECOST: External Knowledge Guided Data-efficient Instruction Tuning
**Authors:** Qi Zhang, Yiming Zhang, Haobo Wang, Junbo Zhao  
**Venue:** Annual Meeting of the Association for Computational Linguistics (ACL) 2024  
**Paper:** [Link needed]  
**Code:** [Link needed]  
**Summary:** External knowledge-guided approach for data-efficient instruction tuning.

---

## Information-Theoretic Methods

### MIG: Automatic Data Selection for Instruction Tuning by Maximizing Information Gain in Semantic Space
**Authors:** Yicheng Chen, Yining Li, Kai Hu, Zerun Ma, Haochen Ye, Kai Chen  
**Venue:** arXiv 2025  
**Paper:** [Link needed]  
**Code:** [Link needed]  
**Summary:** Information gain maximization in semantic space for automatic data selection.

### Instruction Mining: Instruction Data Selection for Tuning Large Language Models
**Authors:** Yihan Cao, Yanbin Kang, Chi Wang, Lichao Sun  
**Venue:** arXiv 2023  
**Paper:** [Link needed]  
**Code:** [Link needed]  
**Summary:** Mining approach for instruction data selection in large language models.

---

## Reward-Based Methods

### ROSE: A Reward-Oriented Data Selection Framework for LLM Task-Specific Instruction Tuning
**Authors:** Yang Wu, Huayi Zhang, Yizheng Jiao, Lin Ma, Xiaozhong Liu, Jinhong Yu, Dongyu Zhang, Dezhi Yu, Wei Xu  
**Venue:** arXiv 2024  
**Paper:** [Link needed]  
**Code:** [Link needed]  
**Summary:** Reward-oriented framework for task-specific instruction tuning data selection.

---

## Efficiency and Performance

### DELIFT: Data Efficient Language model Instruction Fine Tuning
**Authors:** Ishika Agarwal, Krishnateja Killamsetty, Lucian Popa, Marina Danilevsky  
**Venue:** International Conference on Learning Representations 2024  
**Paper:** [Link needed]  
**Code:** [Link needed]  
**Summary:** Data efficient approach to language model instruction fine-tuning.

### Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models
**Authors:** Ziche Liu, Rui Ke, Feng Jiang, Haizhou Li  
**Venue:** arXiv 2024  
**Paper:** [Link needed]  
**Code:** [Link needed]  
**Summary:** Rethinking data selection strategies for fine-tuning large language models.

### The Best Instruction-Tuning Data are Those That Fit
**Authors:** Dylan Zhang, Qirun Dai, Hao Peng  
**Venue:** arXiv 2025  
**Paper:** [Link needed]  
**Code:** [Link needed]  
**Summary:** Analysis of what constitutes the best instruction-tuning data.

### Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning
**Authors:** Haowen Chen, Yining Zhang, Qi Zhang, Hantao Yang, Xiaomeng Hu, Xuetao Ma, Yifan YangGong, J. Zhao  
**Venue:** arXiv 2024  
**Paper:** [Link needed]  
**Code:** [Link needed]  
**Summary:** Explores the possibility of effective instruction tuning with minimal training data.
